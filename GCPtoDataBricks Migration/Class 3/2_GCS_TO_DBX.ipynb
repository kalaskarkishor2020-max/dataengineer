{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2853a2a9-8f5d-44b5-a813-99b630bbb50f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# STEPS TO FOLLOW \n",
    "\n",
    "- Pick the Active Elibile Tables from config Table where active_flag=1 and load_flag=1\n",
    "- And bq_to_gcs_status='COMPLETED'\n",
    "- Reads the Latest PARQUET from GCS (gcs_path/dt=/*.parquet)--> uses the Latest dt Folder\n",
    "- Writes to the Delta bronze path\n",
    "- Create the Table From bronze path top bronze Dataset\n",
    "- Update the config table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fed942ac-6819-43a7-afaa-2791ca68c475",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Widgets for Parametrization"
    }
   },
   "outputs": [],
   "source": [
    "# Create Databricks widgets for MySQL and GCP/Databricks parameters\n",
    "dbutils.widgets.text(\"mysql_host\",     \"136.113.73.13\")  # MySQL server host/IP\n",
    "dbutils.widgets.text(\"mysql_port\",     \"3306\")           # MySQL server port\n",
    "dbutils.widgets.text(\"mysql_db\",       \"GCPmigrationMeta\")  # MySQL database name\n",
    "dbutils.widgets.text(\"mysql_user\",     \"root\")           # MySQL username\n",
    "dbutils.widgets.text(\"mysql_password\", \"Admin@1234\")     # MySQL password (use secrets in production)\n",
    "dbutils.widgets.text(\"GOOGLE_APPLICATION_CREDENTIALS\", \"dbfs:/FileStore/shared_uploads/anurag.srivastava@koantekorg.onmicrosoft.com/datamigrationproject_475415_ffc1c7e9a773.json\")  # GCP service account key path\n",
    "dbutils.widgets.text(\"hive_db\",        \"bronze\")         # Hive database name for Delta tables\n",
    "\n",
    "# Retrieve widget values for use in the notebook\n",
    "mysql_host = dbutils.widgets.get(\"mysql_host\")\n",
    "mysql_port = dbutils.widgets.get(\"mysql_port\")\n",
    "mysql_db = dbutils.widgets.get(\"mysql_db\")\n",
    "mysql_user = dbutils.widgets.get(\"mysql_user\")\n",
    "mysql_password = dbutils.widgets.get(\"mysql_password\")\n",
    "GOOGLE_APPLICATION_CREDENTIALS = dbutils.widgets.get(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "hive_db = dbutils.widgets.get(\"hive_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "722e08f6-a0b8-4706-94ce-ed8dbbe10d19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q mysql-connector-python google-cloud-bigquery google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdea1f6f-c741-4565-98a0-f045672dfe6a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "importing Library for Connection"
    }
   },
   "outputs": [],
   "source": [
    "# If needed once per cluster:\n",
    "# %pip install mysql-connector-python google-cloud-storage\n",
    "\n",
    "# COMMAND ----------\n",
    "import os, re, shutil, json\n",
    "import mysql.connector as mc\n",
    "from contextlib import contextmanager\n",
    "from urllib.parse import urlparse\n",
    "from google.cloud import storage\n",
    "from pyspark.sql.functions import col, to_timestamp, to_date\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# ---------------- Configuration ----------------\n",
    "# Read MySQL and GCP/Databricks parameters from widgets\n",
    "MYSQL = {\n",
    "    \"host\": dbutils.widgets.get(\"mysql_host\").strip(),\n",
    "    \"port\": int(dbutils.widgets.get(\"mysql_port\")),\n",
    "    \"db\":   dbutils.widgets.get(\"mysql_db\").strip(),\n",
    "    \"user\": dbutils.widgets.get(\"mysql_user\").strip(),\n",
    "    \"pwd\":  dbutils.widgets.get(\"mysql_password\"),\n",
    "}\n",
    "# GCP service account key path (convert dbfs:/ to /dbfs/)\n",
    "GCP_KEY = dbutils.widgets.get(\"GOOGLE_APPLICATION_CREDENTIALS\").replace(\"dbfs:/\", \"/dbfs/\")\n",
    "# Hive database name for Delta tables\n",
    "HIVE_DB = dbutils.widgets.get(\"hive_db\").strip()\n",
    "\n",
    "\n",
    "assert os.path.exists(GCP_KEY), f\"GCP key not found at {GCP_KEY}\"\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = GCP_KEY  # auth for GCS SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be403fc2-09c6-4718-aced-ae407b3be15e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setting up MYSQL Connection"
    }
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def mysql_conn():\n",
    "    \"\"\"Context manager for MySQL connection.\"\"\"\n",
    "    conn = mc.connect(\n",
    "        host=MYSQL[\"host\"], port=MYSQL[\"port\"],\n",
    "        user=MYSQL[\"user\"], password=MYSQL[\"pwd\"], database=MYSQL[\"db\"]\n",
    "    )\n",
    "    try:\n",
    "        yield conn\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "654a05f6-ad20-4fd4-ad2d-cb54f071a942",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fetch the Eligible Table"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_eligible_rows():\n",
    "    \"\"\"\n",
    "    Fetch rows from config_table that are eligible for processing:\n",
    "    - active_flag=1\n",
    "    - load_flag=1\n",
    "    - bq_to_gcs_status='COMPLETED'\n",
    "    - gcs_to_bronze_status in ('NOT_STARTED','FAILED')\n",
    "    \"\"\"\n",
    "    with mysql_conn() as conn:\n",
    "        cur = conn.cursor(dictionary=True)\n",
    "        cur.execute(\"\"\"\n",
    "          SELECT table_name, gcs_path, target_path\n",
    "          FROM config_table\n",
    "          WHERE active_flag=1\n",
    "            AND load_flag=1\n",
    "            AND bq_to_gcs_status='COMPLETED'\n",
    "            AND gcs_to_bronze_status IN ('NOT_STARTED','FAILED')\n",
    "          ORDER BY table_name\n",
    "        \"\"\")\n",
    "        rows = cur.fetchall()\n",
    "        cur.close()\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb0d05f9-d435-4010-9c59-fda2ae7af548",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SET BRONZE Status"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def set_bronze_status(table_name, status, err=None):\n",
    "    \"\"\"\n",
    "    Update gcs_to_bronze_status in config_table for a given table.\n",
    "    Status can be 'IN_PROGRESS', 'COMPLETED', or 'FAILED'.\n",
    "    \"\"\"\n",
    "    with mysql_conn() as conn:\n",
    "        cur = conn.cursor()\n",
    "        if status == \"IN_PROGRESS\":\n",
    "            cur.execute(\"\"\"\n",
    "              UPDATE config_table\n",
    "                 SET gcs_to_bronze_status='IN_PROGRESS', last_run_ts=NOW(), error_message=NULL\n",
    "               WHERE table_name=%s\n",
    "            \"\"\", (table_name,))\n",
    "        elif status == \"COMPLETED\":\n",
    "            cur.execute(\"\"\"\n",
    "              UPDATE config_table\n",
    "                 SET gcs_to_bronze_status='COMPLETED', last_success_ts=NOW()\n",
    "               WHERE table_name=%s\n",
    "            \"\"\", (table_name,))\n",
    "        else:  # FAILED\n",
    "            cur.execute(\"\"\"\n",
    "              UPDATE config_table\n",
    "                 SET gcs_to_bronze_status='FAILED', error_message=%s\n",
    "               WHERE table_name=%s\n",
    "            \"\"\", (str(err)[:2000] if err else \"FAILED\", table_name))\n",
    "        conn.commit(); cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ca1bc60-7927-4023-9443-c59692a8b268",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "RESET the LOAD Flag"
    }
   },
   "outputs": [],
   "source": [
    "def reset_load_flag(table_name):\n",
    "    \"\"\"Set load_flag=0 for a table after processing is done.\"\"\"\n",
    "    with mysql_conn() as conn:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"UPDATE config_table SET load_flag=0 WHERE table_name=%s\", (table_name,))\n",
    "        conn.commit(); cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63c20e4b-95ad-4765-879d-585f8ef4a9e8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating GCS Client"
    }
   },
   "outputs": [],
   "source": [
    "def _gcs_client():\n",
    "    \"\"\"Create and return a Google Cloud Storage client.\"\"\"\n",
    "    # env var already set above\n",
    "    return storage.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1119333-3774-4e23-a4a3-8395f7790d31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Sample URI**\n",
    "\n",
    "gs://bigquerytogcsmigration/exports/ods/ods_order_items/dt=20251101T104630Z/000000000000.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efad7358-6117-43e7-9b79-884c3ce5d101",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Split the URI of GCS Bucket"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def _split_gs(gcs_uri: str):\n",
    "    \"\"\"\n",
    "    Split a GCS URI (gs://bucket/path) into (bucket, path).\n",
    "    \"\"\"\n",
    "    assert gcs_uri.startswith(\"gs://\")\n",
    "    p = urlparse(gcs_uri)\n",
    "    return p.netloc, p.path.lstrip(\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c02cca6-f439-4064-a60a-4d7c70a2371e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def latest_dt_uri(gcs_base: str) -> str:\n",
    "    \"\"\"\n",
    "    Find the latest dt=YYYYMMDDTHHMMSSZ folder under gcs_base.\n",
    "    Return a wildcard URI for all Parquet files in that folder.\n",
    "    \"\"\"\n",
    "    bucket_name, base_prefix = _split_gs(gcs_base.rstrip(\"/\"))\n",
    "    cli = _gcs_client()\n",
    "\n",
    "    # List all objects under base_prefix/dt=\n",
    "    search_prefix = f\"{base_prefix}/dt=\"\n",
    "    dts = set()\n",
    "    for blob in cli.list_blobs(bucket_name, prefix=search_prefix):\n",
    "        # Look for '.../dt=xxxxx/' in blob.name or files under it\n",
    "        m = re.search(r\"dt=([\\dT]+Z)/\", blob.name)\n",
    "        if m:\n",
    "            dts.add(m.group(1))\n",
    "    if not dts:\n",
    "        # fallback: allow wildcard if no dt folders found\n",
    "        return f\"gs://{bucket_name}/{base_prefix}/dt=*/*.parquet\"\n",
    "    latest = sorted(dts)[-1]\n",
    "    return f\"gs://{bucket_name}/{base_prefix}/dt={latest}/*.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f84a37e-5df3-4daa-bcba-5978e5bbe7a8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check existence of directory"
    }
   },
   "outputs": [],
   "source": [
    "def _ensure_dir(local_path: str):\n",
    "    \"\"\"Create a local directory if it does not exist.\"\"\"\n",
    "    os.makedirs(local_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f729911c-a093-480e-8817-7652905970d1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function to get the wildcard understanding"
    }
   },
   "outputs": [],
   "source": [
    "def _prefix_from_wildcard(gcs_uri: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Given a GCS URI with wildcard, return (bucket, prefix) for listing.\n",
    "    Example: gs://bucket/foo/bar/dt=.../*.parquet -> (bucket, 'foo/bar/dt=.../')\n",
    "    \"\"\"\n",
    "    bucket, path = _split_gs(gcs_uri)\n",
    "    if \"*\" in path:\n",
    "        prefix = path[:path.rfind(\"/\") + 1]\n",
    "    else:\n",
    "        prefix = path if path.endswith(\"/\") else path + \"/\"\n",
    "    return bucket, prefix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "175f39a5-a69d-49d2-873a-7abb82258569",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "GCS to DBFS Copy of prefix"
    }
   },
   "outputs": [],
   "source": [
    "def copy_gcs_prefix_to_dbfs(gcs_uri_wildcard: str, dbfs_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    Copy all objects under the wildcard's parent prefix from GCS to DBFS directory.\n",
    "    Returns the DBFS directory containing the downloaded Parquet files.\n",
    "    \"\"\"\n",
    "    bucket, prefix = _prefix_from_wildcard(gcs_uri_wildcard)\n",
    "    cli = _gcs_client()\n",
    "    bkt = cli.bucket(bucket)\n",
    "\n",
    "    local_dir = dbfs_dir.replace(\"dbfs:/\", \"/dbfs/\")\n",
    "    if os.path.exists(local_dir):\n",
    "        shutil.rmtree(local_dir)\n",
    "    _ensure_dir(local_dir)\n",
    "\n",
    "    n = 0\n",
    "    for blob in cli.list_blobs(bucket, prefix=prefix):\n",
    "        if blob.name.endswith(\"/\"):\n",
    "            continue\n",
    "        local_file = os.path.join(local_dir, os.path.basename(blob.name))\n",
    "        blob.download_to_filename(local_file)\n",
    "        n += 1\n",
    "    if n == 0:\n",
    "        raise FileNotFoundError(f\"No objects found under gs://{bucket}/{prefix}\")\n",
    "    print(f\"↳ Copied {n} file(s) from gs://{bucket}/{prefix} → {dbfs_dir}\")\n",
    "    return dbfs_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c8610c5-53eb-47a7-abd5-ecffcd22a702",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Convert Path to DBFS format if not already in DBFS."
    }
   },
   "outputs": [],
   "source": [
    "def to_dbfs(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert a path to DBFS format if not already in DBFS.\n",
    "    \"\"\"\n",
    "    p = path.strip()\n",
    "    if p.startswith(\"dbfs:/\") or p.startswith(\"dbfs:\"):\n",
    "        return p\n",
    "    if p.startswith(\"/\"):\n",
    "        return \"dbfs:\" + p\n",
    "    return \"dbfs:/\" + p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5512526-4b32-4b7d-a820-342d1da575af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MAIN ETL LOGIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ca7057b-d840-4636-bb50-9222029c1100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "rows = fetch_eligible_rows()\n",
    "if not rows:\n",
    "    print(\"Nothing to process (eligible rows not found).\")\n",
    "else:\n",
    "    # Ensure Hive database exists for Delta tables\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {HIVE_DB}\")\n",
    "    print(f\"Processing {len(rows)} table(s): {[r['table_name'] for r in rows]}\")\n",
    "    for r in rows:\n",
    "        t = r[\"table_name\"]\n",
    "        gcs_base = r[\"gcs_path\"]\n",
    "        dst_path = to_dbfs(r[\"target_path\"])\n",
    "        try:\n",
    "            set_bronze_status(t, \"IN_PROGRESS\")\n",
    "\n",
    "            # 1) Pick latest dt folder in GCS for this table\n",
    "            src_uri = latest_dt_uri(gcs_base)\n",
    "            print(f\"\\n{t}: staging {src_uri}\")\n",
    "\n",
    "            # 2) Copy Parquet files from GCS to DBFS staging directory\n",
    "            stage_dir = f\"dbfs:/tmp/gcs_stage/{t}\"\n",
    "            stage_dir = copy_gcs_prefix_to_dbfs(src_uri, stage_dir)\n",
    "\n",
    "            # 3) Read Parquet files from DBFS and write to Delta table (overwrite mode)\n",
    "            df = spark.read.parquet(stage_dir)\n",
    "            # --- Normalize time columns (minimal) ---\n",
    "            # Force 'order_ts' to proper TIMESTAMP regardless of source variation\n",
    "            if 'order_ts' in df.columns:\n",
    "                # This safely handles string, date, long (epoch sec/ms if you already pre-converted),\n",
    "                # and passes through if it's already timestamp.\n",
    "                df = df.withColumn('order_ts', to_timestamp(col('order_ts')))\n",
    "\n",
    "            df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\").save(dst_path)\n",
    "\n",
    "            # 4) Register Delta table in Hive (bronze DB)\n",
    "            spark.sql(f\"CREATE TABLE IF NOT EXISTS {HIVE_DB}.`{t}` USING DELTA LOCATION '{dst_path}'\")\n",
    "\n",
    "            set_bronze_status(t, \"COMPLETED\")\n",
    "            reset_load_flag(t)\n",
    "            print(f\"✅ {t}: Bronze written at {dst_path}\")\n",
    "        except Exception as e:\n",
    "            set_bronze_status(t, \"FAILED\", err=e)\n",
    "            print(f\"❌ {t}: {e}\")\n",
    "\n",
    "# (Optional) Clean up DBFS staging directory after run\n",
    "dbutils.fs.rm(\"dbfs:/tmp/gcs_stage\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0339943b-feb0-4414-b9c2-f2322f5cfb2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6068290751876828,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2_GCS_TO_DBX",
   "widgets": {
    "GOOGLE_APPLICATION_CREDENTIALS": {
     "currentValue": "dbfs:/FileStore/shared_uploads/anurag.srivastava@koantekorg.onmicrosoft.com/datamigrationproject_475415_ffc1c7e9a773.json",
     "nuid": "7e518c0d-1d48-434e-afbf-a54f6b889747",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dbfs:/FileStore/shared_uploads/anurag.srivastava@koantekorg.onmicrosoft.com/datamigrationproject_475415_ffc1c7e9a773.json",
      "label": null,
      "name": "GOOGLE_APPLICATION_CREDENTIALS",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dbfs:/FileStore/shared_uploads/anurag.srivastava@koantekorg.onmicrosoft.com/datamigrationproject_475415_ffc1c7e9a773.json",
      "label": null,
      "name": "GOOGLE_APPLICATION_CREDENTIALS",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "bq_sa_key": {
     "currentValue": "/Workspace/Users/anurag.srivastava@koantekorg.onmicrosoft.com/Migrationproject/datamigrationproject-475415-ffc1c7e9a773.json",
     "nuid": "c10c3fea-ecb9-404e-b9e3-c453170e1b65",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Workspace/Users/anurag.srivastava@koantekorg.onmicrosoft.com/Migrationproject/datamigrationproject-475415-ffc1c7e9a773.json",
      "label": null,
      "name": "bq_sa_key",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Workspace/Users/anurag.srivastava@koantekorg.onmicrosoft.com/Migrationproject/datamigrationproject-475415-ffc1c7e9a773.json",
      "label": null,
      "name": "bq_sa_key",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "hive_db": {
     "currentValue": "DataX_GCSMigration",
     "nuid": "26373d45-2f78-481b-b085-781747f3f3b8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "bronze",
      "label": null,
      "name": "hive_db",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "bronze",
      "label": null,
      "name": "hive_db",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "mysql_db": {
     "currentValue": "GCPMigrationMeta",
     "nuid": "2e062977-485f-4708-a0e8-a6e98a234c7f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "GCPmigrationMeta",
      "label": null,
      "name": "mysql_db",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "GCPmigrationMeta",
      "label": null,
      "name": "mysql_db",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "mysql_host": {
     "currentValue": "136.113.73.13",
     "nuid": "47cec4e5-ca85-460a-8340-6d0ce6c4a254",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "136.113.73.13",
      "label": null,
      "name": "mysql_host",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "136.113.73.13",
      "label": null,
      "name": "mysql_host",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "mysql_password": {
     "currentValue": "Admin@1234",
     "nuid": "1fcde268-10b0-4345-a2c7-543785e9e4ea",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Admin@1234",
      "label": null,
      "name": "mysql_password",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "Admin@1234",
      "label": null,
      "name": "mysql_password",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "mysql_port": {
     "currentValue": "3306",
     "nuid": "9d3fe00c-03d8-497c-8966-837d7f8a9a98",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "3306",
      "label": null,
      "name": "mysql_port",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "3306",
      "label": null,
      "name": "mysql_port",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "mysql_user": {
     "currentValue": "root",
     "nuid": "6781e294-bf71-44e1-b28f-92da8e112d8c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "root",
      "label": null,
      "name": "mysql_user",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "root",
      "label": null,
      "name": "mysql_user",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
